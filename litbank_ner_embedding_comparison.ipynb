{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Summary and Key Findings\n\nThis notebook compared two embedding strategies for BiLSTM-based Named Entity Recognition on literary texts:\n\n### Research Design\n\n**Models Compared**:\n1. **BiLSTM with Random Embeddings**: Embeddings learned from scratch\n2. **BiLSTM with GloVe Embeddings**: Pre-trained embeddings fine-tuned during training\n\n**Key Features**:\n- Identical architecture (BiLSTM with same hyperparameters)\n- Same training procedure and data splits\n- Only difference: embedding initialization\n\n### Expected Findings\n\n**Hypothesis**: Pre-trained GloVe embeddings should provide:\n- Better initial representations of word meanings\n- Faster convergence during training\n- Better performance on rare words and literary language\n- Overall higher F1 scores on NER tasks\n\n### Interpretation Guidelines\n\n**Comparing the Models**:\n- **F1 Score**: Primary metric (balances precision and recall)\n- **Training Curves**: Shows learning speed and convergence\n- **Per-Entity Performance**: Some entity types may benefit more from pre-trained embeddings\n- **Generalization**: Compare validation vs. test performance\n\n**Factors to Consider**:\n- Literary texts contain archaic language and unusual names\n- Pre-trained embeddings may not cover all literary vocabulary\n- Fine-tuning allows adaptation to domain-specific language\n- Model capacity (hidden_dim, num_layers) affects both models equally\n\n### Next Steps and Extensions\n\n1. **Experiment with hyperparameters**:\n   - Try different hidden dimensions (64, 256, 512)\n   - Adjust number of LSTM layers (1, 3, 4)\n   - Vary dropout rates (0.1, 0.5)\n   - Test different learning rates (0.0001, 0.01)\n\n2. **Try other embedding strategies**:\n   - FastText embeddings (handle out-of-vocabulary words better)\n   - Domain-specific embeddings trained on literature\n   - Contextual embeddings (ELMo, BERT)\n\n3. **Architectural improvements**:\n   - Add CRF (Conditional Random Field) layer for better entity boundary detection\n   - Implement attention mechanisms\n   - Try character-level embeddings combined with word embeddings\n\n4. **Data augmentation**:\n   - Synthesize more training examples\n   - Use transfer learning from other NER datasets\n   - Implement active learning for efficient annotation\n\n5. **Detailed error analysis**:\n   - Analyze which entity types are most difficult\n   - Study errors on rare vs. common words\n   - Identify systematic failures\n\n### Files Generated\n\nAll results are saved in the `../results/` directory:\n- `BiLSTM_Random_best.pt`: Best random embedding model weights\n- `BiLSTM_GloVe_best.pt`: Best GloVe embedding model weights\n- `embedding_comparison.csv`: Comparison table\n- `embedding_comparison_results.json`: Complete results and configuration\n- `training_curves_embedding_comparison.png`: Training progress visualization\n- `embedding_comparison_charts.png`: Performance comparison charts\n\n### Reproducibility\n\nAll hyperparameters and random seeds are documented in this notebook for full reproducibility. You can easily modify any hyperparameter and re-run the experiments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create comprehensive results summary\nresults_summary = {\n    'experiment': {\n        'name': 'LitBank NER: Embedding Comparison',\n        'description': 'Comparison of random vs pre-trained GloVe embeddings for BiLSTM-based NER',\n        'date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n    },\n    'dataset': {\n        'name': 'LitBank (split_0)',\n        'source': 'coref-data/litbank_raw',\n        'train_sentences': len(train_data),\n        'val_sentences': len(val_data),\n        'test_sentences': len(test_data),\n        'num_tags': len(tag_names),\n        'vocab_size': len(vocab),\n        'tag_names': tag_names\n    },\n    'hyperparameters': {\n        # Model architecture\n        'embedding_dim': EMBEDDING_DIM,\n        'hidden_dim': HIDDEN_DIM,\n        'num_layers': NUM_LAYERS,\n        'dropout': DROPOUT,\n        'bidirectional': True,\n        \n        # Data processing\n        'max_len': MAX_LEN,\n        'batch_size': BATCH_SIZE,\n        'min_word_freq': 2,\n        \n        # Training\n        'num_epochs': NUM_EPOCHS,\n        'learning_rate': LEARNING_RATE,\n        'optimizer': 'Adam',\n        'loss_function': 'CrossEntropyLoss',\n        \n        # GloVe configuration\n        'glove_path': GLOVE_PATH if 'GLOVE_PATH' in locals() else None,\n        'freeze_glove': FREEZE_GLOVE if 'FREEZE_GLOVE' in locals() else None\n    },\n    'models': {}\n}\n\n# Add model results\nfor model_name in training_results.keys():\n    # Get model configuration\n    model = training_results[model_name]['model']\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    # Get test metrics\n    test_metrics = {\n        'f1': float(final_results[model_name]['f1']),\n        'precision': float(final_results[model_name]['precision']),\n        'recall': float(final_results[model_name]['recall']),\n        'loss': float(final_results[model_name]['loss'])\n    }\n    \n    # Get training history\n    history = training_results[model_name]['history']\n    \n    # Store all information\n    results_summary['models'][model_name] = {\n        'architecture': {\n            'total_parameters': total_params,\n            'trainable_parameters': trainable_params,\n            'embedding_type': 'GloVe' if 'GloVe' in model_name else 'Random'\n        },\n        'test_metrics': test_metrics,\n        'best_val_f1': float(training_results[model_name]['best_f1']),\n        'training_history': {\n            'train_loss': [float(x) for x in history['train_loss']],\n            'val_loss': [float(x) for x in history['val_loss']],\n            'val_f1': [float(x) for x in history['val_f1']],\n            'val_precision': [float(x) for x in history['val_precision']],\n            'val_recall': [float(x) for x in history['val_recall']]\n        }\n    }\n\n# Save to JSON\nos.makedirs('../results', exist_ok=True)\nresults_path = '../results/embedding_comparison_results.json'\nwith open(results_path, 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint(f\\\"{'='*80}\\\")\nprint(\\\"RESULTS SAVED\\\")\nprint(f\\\"{'='*80}\\\")\nprint(f\\\"\\\\nResults saved to: {results_path}\\\")\nprint(f\\\"\\\\nSummary:\\\")\nprint(f\\\"  Dataset: {results_summary['dataset']['name']}\\\")\nprint(f\\\"  Models trained: {len(results_summary['models'])}\\\")\nprint(f\\\"  Training epochs: {results_summary['hyperparameters']['num_epochs']}\\\")\nprint(f\\\"\\\\nTest Set Performance:\\\")\nfor model_name in results_summary['models'].keys():\n    metrics = results_summary['models'][model_name]['test_metrics']\n    print(f\\\"  {model_name:20s}: F1={metrics['f1']:.4f}, \\\", end=\\\"\\\")\n    print(f\\\"Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\\\")\nprint(f\\\"\\\\n{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 17. Save Results Summary\n\nSave all results to JSON for reproducibility and later reference. This includes:\n- Dataset statistics\n- Model configurations\n- Hyperparameters used\n- Final test metrics\n- Training history",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def predict_ner(model, text, word2idx, idx2tag, device, max_len=128):\n    \"\"\"\n    Predict NER tags for custom text.\n    \n    Args:\n        model: Trained BiLSTM model\n        text: Input text string\n        word2idx: Word to index mapping\n        idx2tag: Index to tag mapping\n        device: torch device (CPU or GPU)\n        max_len: Maximum sequence length (default=128)\n    \n    Returns:\n        List of (token, tag) tuples\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Tokenize (simple whitespace split)\n    # Note: In production, you'd use a more sophisticated tokenizer\n    tokens = text.split()\n    \n    # Convert tokens to indices\n    # Use <UNK> index for words not in vocabulary\n    token_ids = [\n        word2idx.get(token.lower(), word2idx['<UNK>']) \n        for token in tokens\n    ]\n    \n    # Pad/truncate to max_len\n    if len(token_ids) > max_len:\n        token_ids = token_ids[:max_len]\n        tokens = tokens[:max_len]\n    else:\n        padding_len = max_len - len(token_ids)\n        token_ids = token_ids + [word2idx['<PAD>']] * padding_len\n    \n    # Create attention mask\n    attention_mask = [1 if tid != word2idx['<PAD>'] else 0 for tid in token_ids]\n    \n    # Convert to tensors and add batch dimension\n    input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n    attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n    \n    # Predict\n    with torch.no_grad():\n        logits = model(input_ids, attention_mask_tensor)\n        predictions = torch.argmax(logits, dim=-1)\n    \n    # Convert predictions to tags\n    pred_tags = [\n        idx2tag.get(pred.item(), 'O') \n        for pred in predictions[0][:len(tokens)]\n    ]\n    \n    return list(zip(tokens, pred_tags))\n\n\n# =============================================================================\n# TEST ON SAMPLE LITERARY TEXTS\n# =============================================================================\n# Sample texts from classic literature (include various entity types)\nsample_texts = [\n    \\\"Elizabeth Bennet lived in Longbourn with her family. Mr. Darcy owned Pemberley in Derbyshire.\\\",\n    \\\"Sherlock Holmes resided at 221B Baker Street in London. Dr. Watson often accompanied him.\\\",\n    \\\"Captain Ahab sailed the Pequod across the Atlantic Ocean in pursuit of Moby Dick.\\\",\n    \\\"Jane Eyre arrived at Thornfield Hall to meet Mr. Rochester.\\\",\n    \\\"Emma Woodhouse lived in Highbury near London. She often visited Donwell Abbey.\\\"\n]\n\nprint(f\\\"{'='*80}\\\")\nprint(\\\"TESTING ON CUSTOM LITERARY TEXT\\\")\nprint(f\\\"{'='*80}\\\\n\\\")\n\n# Test each sample text with all models\nfor text_idx, sample_text in enumerate(sample_texts, 1):\n    print(f\\\"\\\\n{'-'*80}\\\")\n    print(f\\\"Sample Text {text_idx}:\\\")\n    print(f\\\"{'-'*80}\\\")\n    print(f\\\"{sample_text}\\\")\n    print()\n    \n    # Get predictions from all models\n    for model_name, results in training_results.items():\n        model = results['model']\n        # Load best model\n        model_path = f'../results/{model_name}_best.pt'\n        if os.path.exists(model_path):\n            model.load_state_dict(torch.load(model_path))\n        model.to(device)\n        \n        # Predict\n        predictions = predict_ner(model, sample_text, word2idx, idx2tag, device, MAX_LEN)\n        \n        # Display results\n        print(f\\\"{model_name}:\\\")\n        # Only show entities (non-'O' tags)\n        entities = [(token, tag) for token, tag in predictions if tag != 'O']\n        if entities:\n            for token, tag in entities:\n                print(f\\\"  {token:25s} → {tag}\\\")\n        else:\n            print(\\\"  (No entities detected)\\\")\n        print()\n\nprint(f\\\"{'-'*80}\\\")\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"CUSTOM TEXT TESTING COMPLETE\\\")\nprint(f\\\"{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Test on Custom Literary Text\n\nTry both models on new literary text to see how they perform in practice.\n\n**Purpose**:\n- Qualitative evaluation: See if the models make sense\n- Compare predictions between random and GloVe embeddings\n- Test on classic literary examples",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CREATE COMPARISON TABLE\n# =============================================================================\n# Extract metrics (excluding predictions and true_labels)\ncomparison_data = {\n    model_name: {k: v for k, v in results.items() if k not in ['predictions', 'true_labels']}\n    for model_name, results in final_results.items()\n}\n\n# Create DataFrame for easy comparison\ncomparison_df = pd.DataFrame(comparison_data).T\ncomparison_df = comparison_df.round(4)\n\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"MODEL COMPARISON SUMMARY\\\")\nprint(f\\\"{'='*80}\\\\n\\\")\nprint(comparison_df)\nprint()\n\n# Calculate and print the improvement/difference\nif len(comparison_df) == 2:\n    models_list = list(comparison_df.index)\n    print(f\\\"\\\\nDifference (GloVe vs Random):\\\")\n    for metric in ['f1', 'precision', 'recall']:\n        if 'BiLSTM_GloVe' in models_list and 'BiLSTM_Random' in models_list:\n            diff = comparison_df.loc['BiLSTM_GloVe', metric] - comparison_df.loc['BiLSTM_Random', metric]\n            pct_change = (diff / comparison_df.loc['BiLSTM_Random', metric]) * 100\n            print(f\\\"  {metric.capitalize():12s}: {diff:+.4f} ({pct_change:+.2f}%)\\\")\n\n# Save to CSV\nos.makedirs('../results', exist_ok=True)\ncomparison_df.to_csv('../results/embedding_comparison.csv')\nprint(f\\\"\\\\nComparison saved to: ../results/embedding_comparison.csv\\\")\n\n# =============================================================================\n# CREATE COMPARISON VISUALIZATIONS\n# =============================================================================\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Bar plot comparing metrics\ncomparison_df[['f1', 'precision', 'recall']].plot(kind='bar', ax=axes[0], rot=0)\naxes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\naxes[0].set_ylabel('Score', fontsize=12)\naxes[0].set_xlabel('Model', fontsize=12)\naxes[0].legend(['F1', 'Precision', 'Recall'], loc='lower right', fontsize=10)\naxes[0].set_ylim([0, 1])\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Radar/spider plot (if we have 2 models)\nif len(comparison_df) >= 1:\n    categories = ['F1', 'Precision', 'Recall']\n    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n    angles += angles[:1]  # Close the circle\n    \n    ax = plt.subplot(122, projection='polar')\n    for model_name in comparison_df.index:\n        values = [\n            comparison_df.loc[model_name, 'f1'],\n            comparison_df.loc[model_name, 'precision'],\n            comparison_df.loc[model_name, 'recall']\n        ]\n        values += values[:1]  # Close the circle\n        ax.plot(angles, values, 'o-', linewidth=2, label=model_name)\n        ax.fill(angles, values, alpha=0.15)\n    \n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(categories, fontsize=11)\n    ax.set_ylim(0, 1)\n    ax.set_title('Performance Metrics Radar Chart', fontsize=14, fontweight='bold', pad=20)\n    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)\n    ax.grid(True)\n\nplt.tight_layout()\nplt.savefig('../results/embedding_comparison_charts.png', dpi=300, bbox_inches='tight')\nprint(f\\\"Comparison charts saved to: ../results/embedding_comparison_charts.png\\\")\nplt.show()\n\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"COMPARISON COMPLETE\\\")\nprint(f\\\"{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 15. Compare Models\n\nCreate comparison tables and visualizations to understand the differences between random and GloVe embeddings.\n\n**What to look for**:\n- Which embedding strategy performs better overall?\n- Are there specific entity types where one approach excels?\n- What's the trade-off between performance and training time?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize loss function for evaluation\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\n# Store final test results\nfinal_results = {}\n\nprint(f\\\"{'='*80}\\\")\nprint(\\\"FINAL EVALUATION ON TEST SET\\\")\nprint(f\\\"{'='*80}\\\\n\\\")\n\n# Evaluate each model\nfor model_name, results in training_results.items():\n    print(f\\\"\\\\n{'-'*80}\\\")\n    print(f\\\"Model: {model_name}\\\")\n    print(f\\\"{'-'*80}\\\")\n    \n    # Load best model weights (saved during training)\n    model = results['model']\n    model_path = f'../results/{model_name}_best.pt'\n    \n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path))\n        print(f\\\"Loaded best model from: {model_path}\\\")\n    else:\n        print(f\\\"Warning: Could not find saved model at {model_path}\\\")\n        print(\\\"Using current model state instead.\\\")\n    \n    model.to(device)\n    \n    # Evaluate on test set\n    test_loss, test_f1, test_precision, test_recall, predictions, true_labels = evaluate(\n        model, test_loader, criterion, device, idx2tag\n    )\n    \n    # Print overall metrics\n    print(f\\\"\\\\nOverall Test Metrics:\\\")\n    print(f\\\"  Test Loss:  {test_loss:.4f}\\\")\n    print(f\\\"  F1 Score:   {test_f1:.4f}\\\")\n    print(f\\\"  Precision:  {test_precision:.4f}\\\")\n    print(f\\\"  Recall:     {test_recall:.4f}\\\")\n    \n    # Print detailed classification report (per-entity metrics)\n    print(f\\\"\\\\nDetailed Classification Report (Per-Entity Metrics):\\\")\n    print(classification_report(true_labels, predictions))\n    \n    # Store results for comparison\n    final_results[model_name] = {\n        'f1': test_f1,\n        'precision': test_precision,\n        'recall': test_recall,\n        'loss': test_loss,\n        'predictions': predictions,\n        'true_labels': true_labels\n    }\n\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"TEST EVALUATION COMPLETE\\\")\nprint(f\\\"{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 14. Final Evaluation on Test Set\n\nLoad the best version of each model and evaluate on held-out test data.\n\n**Why test on a separate set?**\n- Validation set was used for hyperparameter tuning and model selection\n- Test set provides an unbiased estimate of real-world performance\n- Helps detect if we've overfit to the validation set",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a comprehensive visualization of training progress\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Metrics to plot\nmetrics = [\n    ('train_loss', 'Training Loss'),\n    ('val_loss', 'Validation Loss'),\n    ('val_f1', 'Validation F1 Score'),\n    ('val_precision', 'Validation Precision')\n]\n\n# Plot each metric\nfor idx, (metric_key, metric_name) in enumerate(metrics):\n    # Get subplot position (row, col)\n    row = idx // 2\n    col = idx % 2\n    ax = axes[row, col]\n    \n    # Plot line for each model\n    for model_name, results in training_results.items():\n        # Get metric values across epochs\n        values = results['history'][metric_key]\n        epochs = range(1, len(values) + 1)\n        \n        # Plot with marker for each epoch\n        ax.plot(epochs, values, label=model_name, marker='o', markersize=4, linewidth=2)\n    \n    # Customize plot\n    ax.set_xlabel('Epoch', fontsize=11)\n    ax.set_ylabel(metric_name, fontsize=11)\n    ax.set_title(metric_name, fontsize=13, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    # Set appropriate y-axis limits\n    if 'loss' in metric_key.lower():\n        ax.set_ylim(bottom=0)  # Loss starts from 0\n\n# Add overall title\nfig.suptitle('Training Progress: Embedding Comparison', fontsize=16, fontweight='bold', y=1.00)\n\n# Save and show\nplt.tight_layout()\nos.makedirs('../results', exist_ok=True)\nplt.savefig('../results/training_curves_embedding_comparison.png', dpi=300, bbox_inches='tight')\nprint(\\\"Training curves saved to: ../results/training_curves_embedding_comparison.png\\\")\nplt.show()\n\n# Print summary statistics\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"TRAINING SUMMARY\\\")\nprint(f\\\"{'='*80}\\\")\nfor model_name, results in training_results.items():\n    history = results['history']\n    print(f\\\"\\\\n{model_name}:\\\")\n    print(f\\\"  Final train loss:     {history['train_loss'][-1]:.4f}\\\")\n    print(f\\\"  Final val loss:       {history['val_loss'][-1]:.4f}\\\")\n    print(f\\\"  Final val F1:         {history['val_f1'][-1]:.4f}\\\")\n    print(f\\\"  Best val F1:          {results['best_f1']:.4f}\\\")\n    print(f\\\"  Final val precision:  {history['val_precision'][-1]:.4f}\\\")\n    print(f\\\"  Final val recall:     {history['val_recall'][-1]:.4f}\\\")\nprint(f\\\"{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Visualize Training Progress\n\nPlot learning curves to see how models improved over epochs. This helps us understand:\n- Whether models are converging\n- If there's overfitting (train loss decreasing but val loss increasing)\n- How quickly each model learns\n- Comparative performance between random and GloVe embeddings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def train_model(model, train_loader, val_loader, model_name, \n                num_epochs=15, learning_rate=0.001):\n    \"\"\"\n    Train a model for multiple epochs and save the best version.\n    \n    Args:\n        model: The BiLSTM model to train\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        model_name: Name for saving the model (e.g., \"BiLSTM_Random\")\n        num_epochs: Number of training epochs (default=15)\n        learning_rate: Learning rate for Adam optimizer (default=0.001)\n    \n    Returns:\n        history: Dictionary with training metrics per epoch\n        best_f1: Best F1 score achieved on validation set\n    \"\"\"\n    # Move model to device (GPU or CPU)\n    model = model.to(device)\n    \n    # Loss function: Cross Entropy Loss\n    # ignore_index=0 means padding tokens (index 0) are ignored in loss calculation\n    # This prevents the model from being penalized for predicting padding\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    # Optimizer: Adam (Adaptive Moment Estimation)\n    # Adam automatically adjusts learning rate for each parameter\n    # It's generally more robust than SGD and requires less tuning\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Track metrics over epochs for visualization\n    history = {\n        'train_loss': [],      # Training loss per epoch\n        'val_loss': [],        # Validation loss per epoch\n        'val_f1': [],          # Validation F1 score per epoch\n        'val_precision': [],   # Validation precision per epoch\n        'val_recall': []       # Validation recall per epoch\n    }\n    \n    best_f1 = 0  # Track best F1 score for model saving\n    \n    print(f\"\\\\n{'='*80}\")\n    print(f\"Training {model_name}\")\n    print(f\"{'='*80}\")\n    print(f\"Configuration:\")\n    print(f\"  Device: {device}\")\n    print(f\"  Epochs: {num_epochs}\")\n    print(f\"  Learning rate: {learning_rate}\")\n    print(f\"  Batch size: {BATCH_SIZE}\")\n    print(f\"{'='*80}\")\n    \n    # Train for num_epochs epochs\n    for epoch in range(num_epochs):\n        print(f\"\\\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(\\\"-\\\" * 60)\n        \n        # =========================================================================\n        # TRAINING PHASE\n        # =========================================================================\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n        history['train_loss'].append(train_loss)\n        \n        # =========================================================================\n        # EVALUATION PHASE\n        # =========================================================================\n        val_loss, val_f1, val_precision, val_recall, _, _ = evaluate(\n            model, val_loader, criterion, device, idx2tag\n        )\n        history['val_loss'].append(val_loss)\n        history['val_f1'].append(val_f1)\n        history['val_precision'].append(val_precision)\n        history['val_recall'].append(val_recall)\n        \n        # Print metrics for this epoch\n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val   Loss: {val_loss:.4f} | F1: {val_f1:.4f} | \\\"\n              f\\\"Precision: {val_precision:.4f} | Recall: {val_recall:.4f}\")\n        \n        # =========================================================================\n        # MODEL CHECKPOINTING: Save best model based on F1 score\n        # =========================================================================\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            # Create results directory if it doesn't exist\n            os.makedirs('../results', exist_ok=True)\n            # Save model weights\n            model_path = f'../results/{model_name}_best.pt'\n            torch.save(model.state_dict(), model_path)\n            print(f\\\"  ✓ New best model saved! F1: {best_f1:.4f} → {model_path}\")\n    \n    print(f\\\"\\\\n{'='*80}\")\n    print(f\"Training completed for {model_name}!\")\n    print(f\"Best validation F1: {best_f1:.4f}\")\n    print(f\"{'='*80}\\\\n\")\n    \n    return history, best_f1\n\n\n# =============================================================================\n# TRAINING HYPERPARAMETERS: Easy to adjust for experimentation\n# =============================================================================\nNUM_EPOCHS = 15        # Number of epochs to train (more epochs = more training time)\nLEARNING_RATE = 0.001  # Learning rate for Adam optimizer (typical range: 0.0001 to 0.01)\n\n# Store results for all models\ntraining_results = {}\n\nprint(f\"\\\\n{'='*80}\")\nprint(\\\"STARTING TRAINING\\\"\")\nprint(f\\\"{'='*80}\\\")\nprint(f\\\"Training {len(models)} model(s):\\\")\nfor model_name in models.keys():\n    print(f\\\"  - {model_name}\\\")\nprint(f\\\"\\\\nThis may take a while depending on your hardware...\\\")\nprint(f\\\"{'='*80}\\\\n\\\")\n\n# Train each model\nfor model_name, model in models.items():\n    # Train the model and get training history\n    history, best_f1 = train_model(\n        model, \n        train_loader, \n        val_loader, \n        model_name, \n        NUM_EPOCHS, \n        LEARNING_RATE\n    )\n    \n    # Store results for later comparison\n    training_results[model_name] = {\n        'history': history,     # Training metrics per epoch\n        'best_f1': best_f1,     # Best F1 score on validation set\n        'model': model          # The trained model\n    }\n\nprint(f\\\"\\\\n{'='*80}\\\")\nprint(\\\"ALL MODELS TRAINED SUCCESSFULLY!\\\")\nprint(f\\\"{'='*80}\\\")\nprint(\\\"\\\\nBest validation F1 scores:\\\")\nfor model_name, results in training_results.items():\n    print(f\\\"  {model_name:20s}: {results['best_f1']:.4f}\\\")\nprint(f\\\"{'='*80}\\\\n\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Training Loop\n\nTrain both models (Random and GloVe embeddings) and track their progress.\n\n**Training Configuration** (easily adjustable):\n- Number of epochs\n- Learning rate\n- Optimizer settings\n- Model checkpoint saving\n\nEach model is trained independently and the best version (based on validation F1) is saved.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def train_epoch(model, dataloader, optimizer, criterion, device):\n    \"\"\"\n    Train the model for one epoch.\n    \n    Args:\n        model: The BiLSTM model to train\n        dataloader: DataLoader for training data\n        optimizer: Optimizer (e.g., Adam)\n        criterion: Loss function (e.g., CrossEntropyLoss)\n        device: torch device (CPU or GPU)\n    \n    Returns:\n        float: Average loss for this epoch\n    \"\"\"\n    # Set model to training mode\n    # This enables dropout and other training-specific behaviors\n    model.train()\n    \n    total_loss = 0  # Accumulate loss over all batches\n    \n    # Loop through batches with progress bar\n    for batch in tqdm(dataloader, desc='Training', leave=False):\n        # Move batch tensors to device (GPU or CPU)\n        input_ids = batch['input_ids'].to(device)        # Token indices\n        labels = batch['labels'].to(device)              # True NER tags\n        attention_mask = batch['attention_mask'].to(device)  # Padding mask\n        \n        # Reset gradients from previous iteration\n        # PyTorch accumulates gradients, so we need to zero them out\n        optimizer.zero_grad()\n        \n        # Forward pass: Get predictions from model\n        # logits shape: (batch_size, seq_len, num_tags)\n        logits = model(input_ids, attention_mask)\n        \n        # Reshape tensors for loss calculation\n        # CrossEntropyLoss expects:\n        #   - predictions: (N, num_classes) where N = batch_size * seq_len\n        #   - targets: (N,)\n        logits_flat = logits.view(-1, logits.shape[-1])  # (batch_size * seq_len, num_tags)\n        labels_flat = labels.view(-1)                    # (batch_size * seq_len,)\n        \n        # Calculate loss (how wrong are the predictions?)\n        # Padding tokens (label=0) are automatically ignored due to ignore_index=0\n        loss = criterion(logits_flat, labels_flat)\n        \n        # Backward pass: Calculate gradients\n        # This computes the gradient of loss with respect to each parameter\n        loss.backward()\n        \n        # Gradient clipping: Prevent exploding gradients\n        # If gradients are too large, they can cause training instability\n        # We clip them to a maximum norm of 1.0\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        # Update model parameters using gradients\n        optimizer.step()\n        \n        # Accumulate loss for averaging\n        total_loss += loss.item()\n    \n    # Return average loss over all batches\n    return total_loss / len(dataloader)\n\n\ndef evaluate(model, dataloader, criterion, device, idx2tag):\n    \"\"\"\n    Evaluate the model on validation/test data.\n    \n    Args:\n        model: The BiLSTM model to evaluate\n        dataloader: DataLoader for validation/test data\n        criterion: Loss function\n        device: torch device (CPU or GPU)\n        idx2tag: Dictionary mapping tag indices to tag names\n    \n    Returns:\n        tuple: (avg_loss, f1, precision, recall, predictions, true_labels)\n            - avg_loss: Average loss over all batches\n            - f1: F1 score (harmonic mean of precision and recall)\n            - precision: Precision score\n            - recall: Recall score\n            - predictions: List of predicted tag sequences (for analysis)\n            - true_labels: List of true tag sequences (for analysis)\n    \"\"\"\n    # Set model to evaluation mode\n    # This disables dropout and other training-specific behaviors\n    model.eval()\n    \n    total_loss = 0      # Accumulate loss\n    predictions = []    # Store all predictions\n    true_labels = []    # Store all true labels\n    \n    # Disable gradient calculation for evaluation (saves memory and computation)\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Evaluating', leave=False):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            # Forward pass: Get predictions\n            logits = model(input_ids, attention_mask)\n            \n            # Calculate loss\n            logits_flat = logits.view(-1, logits.shape[-1])\n            labels_flat = labels.view(-1)\n            loss = criterion(logits_flat, labels_flat)\n            total_loss += loss.item()\n            \n            # Get predicted tags: Choose tag with highest score for each token\n            # argmax returns the index of the maximum value\n            preds = torch.argmax(logits, dim=-1)  # Shape: (batch_size, seq_len)\n            \n            # Convert predictions and labels to tag names\n            # Only include non-padded tokens (attention_mask == 1)\n            for i in range(len(preds)):\n                pred_tags = []  # Predicted tags for this sequence\n                true_tags = []  # True tags for this sequence\n                \n                # Loop through each token in the sequence\n                for j in range(len(preds[i])):\n                    # Only process real tokens (not padding)\n                    if attention_mask[i][j] == 1:\n                        # Convert index to tag name using idx2tag mapping\n                        pred_idx = preds[i][j].item()\n                        true_idx = labels[i][j].item()\n                        \n                        pred_tags.append(idx2tag.get(pred_idx, 'O'))\n                        true_tags.append(idx2tag.get(true_idx, 'O'))\n                \n                # Only add non-empty sequences\n                if pred_tags:\n                    predictions.append(pred_tags)\n                    true_labels.append(true_tags)\n    \n    # Calculate average loss\n    avg_loss = total_loss / len(dataloader)\n    \n    # Calculate NER metrics using seqeval\n    # seqeval properly handles multi-token entities (e.g., \"B-PER I-PER\")\n    # It treats \"B-PER I-PER\" as a single entity, not two separate tokens\n    f1 = f1_score(true_labels, predictions)           # F1 score\n    precision = precision_score(true_labels, predictions)  # Precision\n    recall = recall_score(true_labels, predictions)        # Recall\n    \n    return avg_loss, f1, precision, recall, predictions, true_labels\n\n\nprint(\"Training and evaluation functions defined successfully!\")\nprint(\"\\\\nThese functions include:\")\nprint(\"  ✓ Automatic handling of padding tokens\")\nprint(\"  ✓ Gradient clipping for stable training\")\nprint(\"  ✓ Progress bars for monitoring\")\nprint(\"  ✓ Proper NER evaluation with seqeval\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Training and Evaluation Functions\n\nThese functions handle:\n- **Training loop**: Forward pass, loss calculation, backward pass, weight updates\n- **Evaluation**: Calculate metrics (F1, precision, recall) using seqeval\n\n**Key Features**:\n- Progress bars for monitoring training\n- Gradient clipping to prevent exploding gradients\n- Proper handling of padding tokens (ignored in loss calculation)\n- Per-entity metrics using seqeval (handles multi-token entities correctly)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LitBank Named Entity Recognition: Embedding Comparison\n",
    "\n",
    "This notebook contains the following steps:\n",
    "1. Loading and exploring the LitBank dataset\n",
    "2. Data preprocessing and flattening for NER\n",
    "3. Loading pre-trained GloVe embeddings\n",
    "4. Building BiLSTM models with different embeddings (random vs GloVe)\n",
    "5. Training and evaluation\n",
    "6. Performance comparison and analysis\n",
    "\n",
    "**Dataset**: LitBank - annotated literary texts with entity labels\n",
    "\n",
    "**Research Question**: How do different word embeddings affect BiLSTM performance on literary NER?\n",
    "\n",
    "**Note**: This version compares:\n",
    "- **Random embeddings**: Randomly initialized, learned from scratch\n",
    "- **GloVe embeddings**: Pre-trained on Common Crawl (840B tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch for deep learning\n",
    "import torch  # Main PyTorch library\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torch.utils.data import Dataset, DataLoader  # Data handling utilities\n",
    "\n",
    "# Import scientific computing libraries\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns  # Statistical visualizations\n",
    "\n",
    "# Import machine learning utilities\n",
    "from sklearn.model_selection import train_test_split  # Data splitting\n",
    "\n",
    "# Import NER evaluation metrics (specialized for sequence labeling)\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Import standard Python libraries\n",
    "from collections import Counter  # Count occurrences\n",
    "from tqdm.auto import tqdm  # Progress bars\n",
    "import os  # Operating system utilities\n",
    "import json  # JSON file handling\n",
    "\n",
    "# Set random seeds for reproducibility (ensures same results each run)\n",
    "torch.manual_seed(42)  # PyTorch random seed\n",
    "np.random.seed(42)  # NumPy random seed\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise CPU\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  # Print GPU name\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")  # Print CUDA version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LitBank Dataset\n",
    "\n",
    "LitBank is loaded from Hugging Face. It contains literary texts annotated with:\n",
    "- Named entities (PER, LOC, FAC, GPE, VEH, ORG)\n",
    "- Coreference chains\n",
    "- Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets library from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load LitBank dataset from Hugging Face\n",
    "try:\n",
    "    # Load the dataset with split_0 configuration\n",
    "    dataset = load_dataset(\"coref-data/litbank_raw\", \"split_0\")\n",
    "    print(\"LitBank dataset loaded successfully!\")\n",
    "    # Display the dataset structure (train/validation/test splits)\n",
    "    print(f\"Dataset structure: {dataset}\")\n",
    "except Exception as e:\n",
    "    # If loading fails, print error message\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"You may need to download LitBank manually from https://github.com/dbamman/litbank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Dataset Structure\n",
    "\n",
    "LitBank has a nested structure. The following cells explore the structure and analyse the contents\n",
    "* there are 100 rows, each corresponding to a separate literary work from 1719 to 1922 (for complete list see https://github.com/dbamman/litbank)\n",
    "* the dataset contains an extract of ~2000 tokens per book, the number of sentences varies according to the sentence length (see sentence length analysis below)\n",
    "* the total annotated dataset contains 210,532 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT-LEVEL VIEW: Show all features from first 5 documents in a table\n",
    "print(\"=\" * 80)\n",
    "print(\"DOCUMENT-LEVEL OVERVIEW - ALL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert first 5 documents to DataFrame\n",
    "df_docs = pd.DataFrame(dataset['train'][:5])\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first example in the training set\n",
    "if 'train' in dataset and len(dataset['train']) > 0:\n",
    "    # Get the first sample from training data\n",
    "    sample = dataset['train'][0]\n",
    "    \n",
    "    # Print all available keys (fields) in the sample\n",
    "    print(\"Available fields in each document:\")\n",
    "    print(list(sample.keys()))\n",
    "    \n",
    "    # Show the structure of sentences (nested list)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SENTENCES structure:\")\n",
    "    print(f\"Number of sentences in first document: {len(sample['sentences'])}\")\n",
    "    print(f\"First sentence tokens: {sample['sentences'][0]}\")\n",
    "    print(f\"Second sentence tokens: {sample['sentences'][1]}\")\n",
    "    \n",
    "    # Show the structure of entities (nested list with dicts)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTITIES structure:\")\n",
    "    print(f\"Number of entity lists: {len(sample['entities'])}\")\n",
    "    print(f\"Entities in first sentence: {sample['entities'][0][:5]}\")  # Show first 5 entities\n",
    "    \n",
    "    # Explain the structure\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRUCTURE EXPLANATION:\")\n",
    "    print(\"- Each document contains multiple sentences\")\n",
    "    print(\"- sentences[i] = list of tokens in sentence i\")\n",
    "    print(\"- entities[i] = list of entity dicts for sentence i\")\n",
    "    print(\"- Each entity dict has: 'token' (word) and 'bio_tags' (NER labels)\")\n",
    "    print(\"- bio_tags can be: 'O' (not an entity) or 'B-TYPE', 'I-TYPE' (entity)\")\n",
    "else:\n",
    "    print(\"No training data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, coreference chains and events are not relevant, only the NER tags (`entities`).\n",
    "\n",
    "Every token has a NER tag that is either 0 (no recognised entity) or one of 6 entities: \n",
    "* Person\n",
    "* Location\n",
    "* Facility\n",
    "* Geopolitical entity\n",
    "* Vehicle\n",
    "* Organisation\n",
    "\n",
    "NB: the annotation allows for nested entities, i.e. one token can be part of several entities (e.g. in `Mary's house` the token `Mary` would be a PER as well as part of an entity LOC). For each token there is a dictionary containing the token itself and the `bio_tags`for several annotation levels. \n",
    "* Layer 0 is used for primary annotation (13.7% of tokens).\n",
    "* Layers 1-2 show nested entity annotations (fairly common, 6% of tokens).\n",
    "* Layer 3 shows deep nesting (very rare - only 0.1% of tokens).\n",
    "* Layer 4 is never used in this dataset.\n",
    "\n",
    "For simplicity in this notebook only the first layer of annotation is used for training. It is true that some of the annotation information is lost. In a more sophisticated approach it would be necessary to use all layers or combine them into  a richer annotation scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flatten Dataset Structure\n",
    "\n",
    "The model needs flat lists of tokens and tags. The following function will convert LitBank's nested structure into sentence-level examples.\n",
    "\n",
    "**Input**: Nested structure, organised by document. The tokens are in feature `sentences` the NER tags in the feature `entities`.  \n",
    "**Output**: Each sentence becomes its own example with a flat structure (`tokens` + `ner_tags`). Each token has exactly one tag (only layer 0 is used, see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_litbank_data(dataset_split):\n",
    "    \"\"\"\n",
    "    Convert LitBank's nested structure to flat sentence-level examples.\n",
    "    \n",
    "    Input: Dataset with nested sentences and entities per document\n",
    "    Output: List of dicts with 'tokens' and 'ner_tags' per sentence\n",
    "    \"\"\"\n",
    "    flattened_examples = []  # Store all flattened sentences here\n",
    "    \n",
    "    # Loop through each document in the dataset\n",
    "    for doc_idx, example in enumerate(dataset_split):\n",
    "        # Get all sentences in this document (list of token lists)\n",
    "        sentences = example.get('sentences', [])\n",
    "        # Get all entities in this document (list of entity lists per sentence)\n",
    "        entities = example.get('entities', [])\n",
    "        \n",
    "        # Process each sentence in the document\n",
    "        for sent_idx, sentence_tokens in enumerate(sentences):\n",
    "            # Skip empty sentences\n",
    "            if not sentence_tokens:\n",
    "                continue\n",
    "            \n",
    "            # Get entities for this specific sentence, entities are aligned with sentences (entities[i] corresponds to sentences[i])\n",
    "            sent_entities = entities[sent_idx] if sent_idx < len(entities) else []\n",
    "            \n",
    "            # Create a mapping from token to its NER tag, each token in sentence_tokens needs a corresponding tag\n",
    "            token_to_tag = {}  # Dictionary: token_index -> NER tag\n",
    "            \n",
    "            # Process each entity in this sentence\n",
    "            for entity_dict in sent_entities:\n",
    "                # Each entity has 'token' (the word) and 'bio_tags' (its NER label, layer 0)\n",
    "                token = entity_dict.get('token', '')\n",
    "                bio_tags = entity_dict.get('bio_tags', [])\n",
    "                \n",
    "                # bio_tags can be a list (for multi-word entities) or single string\n",
    "                if isinstance(bio_tags, list) and len(bio_tags) > 0:\n",
    "                    # Take the first tag if it's a list\n",
    "                    tag = bio_tags[0] if bio_tags[0] else 'O'\n",
    "                elif isinstance(bio_tags, str):\n",
    "                    # Use the tag directly if it's a string\n",
    "                    tag = bio_tags if bio_tags else 'O'\n",
    "                else:\n",
    "                    # Default to 'O' (outside entity) if no tag\n",
    "                    tag = 'O'\n",
    "                \n",
    "                # Find this token in the sentence and record its tag\n",
    "                # We search for the token's position to align it\n",
    "                for token_idx, sent_token in enumerate(sentence_tokens):\n",
    "                    # If token matches and hasn't been tagged yet\n",
    "                    if sent_token == token and token_idx not in token_to_tag:\n",
    "                        token_to_tag[token_idx] = tag\n",
    "                        break  # Found it, move to next entity\n",
    "            \n",
    "            # Create the final list of tags aligned with tokens, any token not in token_to_tag gets 'O' (outside entity)\n",
    "            ner_tags = [token_to_tag.get(i, 'O') for i in range(len(sentence_tokens))]\n",
    "            \n",
    "            # Add this sentence as a flat example\n",
    "            flattened_examples.append({\n",
    "                'tokens': sentence_tokens,  # List of words\n",
    "                'ner_tags': ner_tags,  # List of NER labels (same length)\n",
    "                'doc_idx': doc_idx,  # Track which document it came from\n",
    "                'sent_idx': sent_idx  # Track which sentence in the document\n",
    "            })\n",
    "    \n",
    "    return flattened_examples\n",
    "\n",
    "# Flatten all dataset splits\n",
    "train_data = flatten_litbank_data(dataset['train'])  # Flatten training data\n",
    "val_data = flatten_litbank_data(dataset['validation'])  # Flatten validation data\n",
    "test_data = flatten_litbank_data(dataset['test'])  # Flatten test data\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nFlattened data statistics:\")\n",
    "print(f\"Training sentences: {len(train_data)}\")\n",
    "print(f\"Validation sentences: {len(val_data)}\")\n",
    "print(f\"Test sentences: {len(test_data)}\")\n",
    "\n",
    "# Show an example\n",
    "print(f\"\\nExample sentence:\")\n",
    "print(f\"Tokens: {train_data[0]['tokens'][:15]}...\")  # First 15 tokens\n",
    "print(f\"Tags:   {train_data[0]['ner_tags'][:15]}...\")  # Corresponding tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NER Tag Distribution and Tag Mapping\n",
    "\n",
    "The next few cells analyse the types of entities that appear in LitBank and how frequently. This is important to understand the class imbalance: the data shows a big imbalance, 86% of the tokens are tagged `0` (i.e. no entity). \n",
    "* Choosing the right evaluation metric: accuracy is probably misleading. If the model does not learn to recognise any entity, it will still be correct in 86% of cases. Precision, recall or the F1 score are better. For accuracy, 86% would be the baseline to check if the model has learned anything at all.\n",
    "* Data strategies: Weight the loss function to make the model care more about rare classes (entities) than common ones (O) or oversample minority classes (Show the model more examples of rare entity types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all NER tags in the training data\n",
    "def get_tag_distribution(flattened_data):\n",
    "    \"\"\"Count how many times each NER tag appears.\"\"\"\n",
    "    all_tags = []  # Collect all tags\n",
    "    \n",
    "    # Loop through each sentence\n",
    "    for example in flattened_data:\n",
    "        # Add all tags from this sentence to our list\n",
    "        all_tags.extend(example['ner_tags'])\n",
    "    \n",
    "    # Count occurrences of each tag\n",
    "    return Counter(all_tags)\n",
    "\n",
    "# Get tag distribution from training data\n",
    "tag_dist = get_tag_distribution(train_data)\n",
    "\n",
    "# Print the distribution\n",
    "print(\"\\nNER Tag Distribution:\")\n",
    "for tag, count in sorted(tag_dist.items()):\n",
    "    # Calculate percentage\n",
    "    percentage = (count / sum(tag_dist.values())) * 100\n",
    "    print(f\"{tag:15s}: {count:6d} occurrences ({percentage:5.2f}%)\")\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Sort tags by count for better visualization\n",
    "sorted_tags = sorted(tag_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "tags, counts = zip(*sorted_tags)\n",
    "# Create bar plot\n",
    "plt.bar(range(len(tags)), counts)\n",
    "plt.xticks(range(len(tags)), tags, rotation=45, ha='right')\n",
    "plt.xlabel('NER Tag')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('NER Tag Distribution in LitBank (Training Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show distribution without 'O' tag for better visibility of entities\n",
    "entity_tags = {tag: count for tag, count in tag_dist.items() if tag != 'O'}\n",
    "if entity_tags:\n",
    "    print(\"\\nEntity Tags Only (excluding 'O'):\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_entity_tags = sorted(entity_tags.items(), key=lambda x: x[1], reverse=True)\n",
    "    tags, counts = zip(*sorted_entity_tags)\n",
    "    plt.bar(range(len(tags)), counts, color='steelblue')\n",
    "    plt.xticks(range(len(tags)), tags, rotation=45, ha='right')\n",
    "    plt.xlabel('Entity Tag')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Entity Tag Distribution (Excluding \"O\" Tag)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tag Mappings**\n",
    "\n",
    "Neural networks work with numbers, not text. The following cell creates two dictionaries to \"translate\" the tags to numbers and back again:\n",
    "- `tag2idx`: Tag names (like \"B-PER\") → numbers (like 0, 1, 2...)\n",
    "- `idx2tags`: Numbers → tag names (for interpreting predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique tags from the training data\n",
    "all_tags = set()  # create a set to get unique values only\n",
    "for example in train_data:\n",
    "    # Add all tags from this sentence to the set\n",
    "    all_tags.update(example['ner_tags'])\n",
    "\n",
    "# Convert to a sorted list (put 'O' first, then alphabetical), 'O' should be index 0 because we'll use it for padding\n",
    "tag_names = sorted(all_tags, key=lambda x: (x != 'O', x))\n",
    "\n",
    "# Create mapping: tag name → number (for converting labels to model input)\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_names)}\n",
    "\n",
    "# Create reverse mapping: number → tag name (for converting predictions to readable tags)\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(tag_names)}\n",
    "\n",
    "# Print the mappings\n",
    "print(\"Tag to Index Mapping:\")\n",
    "for tag, idx in tag2idx.items():\n",
    "    print(f\"  {tag:15s} → {idx}\")\n",
    "\n",
    "print(f\"\\nTotal unique tags: {len(tag_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Sentence Lengths and Build Vocabulary\n",
    "\n",
    "Understanding sentence length distribution helps us choose the right maximum sequence length (`MAX_LEN`) for the model (needs inputs of consistent, predictable size).  \n",
    "The hyperparameter `MAX_LEN` will determine how much shorter sentences will be padded or where longer sentences are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of each sentence (number of tokens)\n",
    "sentence_lengths = [len(example['tokens']) for example in train_data]\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"Sentence Length Statistics:\")\n",
    "print(f\"  Mean:   {np.mean(sentence_lengths):.2f} tokens\")\n",
    "print(f\"  Median: {np.median(sentence_lengths):.0f} tokens\")\n",
    "print(f\"  Std:    {np.std(sentence_lengths):.2f} tokens\")\n",
    "print(f\"  Min:    {np.min(sentence_lengths)} tokens\")\n",
    "print(f\"  Max:    {np.max(sentence_lengths)} tokens\")\n",
    "\n",
    "# Calculate percentiles to help choose max_len\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(sentence_lengths, p)\n",
    "    print(f\"  {p}th percentile: {value:.0f} tokens\")\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(sentence_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(sentence_lengths), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(sentence_lengths):.1f}')\n",
    "plt.axvline(np.median(sentence_lengths), color='green', linestyle='--', \n",
    "            label=f'Median: {np.median(sentence_lengths):.0f}')\n",
    "plt.xlabel('Sentence Length (number of tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths in Training Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suggest a good max_len value\n",
    "recommended_max_len = int(np.percentile(sentence_lengths, 95))\n",
    "print(f\"\\nRecommended MAX_LEN: {recommended_max_len} (covers 95% of sentences)\")\n",
    "\n",
    "# Build vocabulary from training data\n",
    "def build_vocab(flattened_data, min_freq=2):\n",
    "    \"\"\"\n",
    "    Build vocabulary from training data.\n",
    "    \n",
    "    Args:\n",
    "        flattened_data: List of sentence dicts with 'tokens'\n",
    "        min_freq: Minimum frequency for a word to be included (default=2)\n",
    "                  Words appearing less than this become <UNK> (unknown)\n",
    "    \n",
    "    Returns:\n",
    "        word2idx: Dictionary mapping words to indices\n",
    "        vocab: List of all words in vocabulary\n",
    "    \"\"\"\n",
    "    # Count how many times each word appears\n",
    "    word_freq = Counter()\n",
    "    for example in flattened_data:\n",
    "        # Convert to lowercase for case-insensitive vocabulary\n",
    "        word_freq.update([token.lower() for token in example['tokens']])\n",
    "    \n",
    "    # Build vocabulary: special tokens first, then frequent words\n",
    "    # <PAD>: Padding token (index 0) - used to make all sequences same length\n",
    "    # <UNK>: Unknown token (index 1) - used for words not in vocabulary\n",
    "    vocab = ['<PAD>', '<UNK>']\n",
    "    \n",
    "    # Add words that appear at least min_freq times\n",
    "    vocab.extend([word for word, freq in word_freq.items() if freq >= min_freq])\n",
    "    \n",
    "    # Create mapping: word → index\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nTotal unique words (before filtering): {len(word_freq)}\")\n",
    "    print(f\"Vocabulary size (min_freq={min_freq}): {len(vocab)}\")\n",
    "    print(f\"Words filtered out: {len(word_freq) - (len(vocab) - 2)}\")\n",
    "    \n",
    "    return word2idx, vocab\n",
    "\n",
    "# Build vocabulary from training data\n",
    "word2idx, vocab = build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Show some example words from vocabulary\n",
    "print(f\"\\nSample vocabulary words: {vocab[2:22]}\")  # Skip <PAD> and <UNK>\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "print(f\"  <PAD> index: {word2idx['<PAD>']}\")\n",
    "print(f\"  <UNK> index: {word2idx['<UNK>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Pre-trained GloVe Embeddings\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) are pre-trained word embeddings that capture semantic relationships between words. We'll use the 100-dimensional GloVe embeddings trained on Common Crawl.\n",
    "\n",
    "**Why GloVe?**\n",
    "- Pre-trained on massive text corpora (840B tokens)\n",
    "- Captures semantic and syntactic relationships\n",
    "- May help with rare words and literary language\n",
    "\n",
    "**Note**: You need to download GloVe embeddings first. Download from: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, word2idx, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Load pre-trained GloVe embeddings and create embedding matrix for our vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        glove_path: Path to GloVe file (e.g., 'glove.6B.100d.txt')\n",
    "        word2idx: Dictionary mapping words to indices in our vocabulary\n",
    "        embedding_dim: Dimension of GloVe vectors (50, 100, 200, or 300)\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: NumPy array of shape (vocab_size, embedding_dim)\n",
    "                         Each row i contains the embedding for word at index i\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe embeddings from: {glove_path}\")\n",
    "    print(f\"This may take a few minutes...\\n\")\n",
    "    \n",
    "    # Initialize embedding matrix with random values (for words not in GloVe)\n",
    "    # We use a small random initialization that will be updated during training\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
    "    \n",
    "    # Set padding token to zeros (index 0)\n",
    "    embedding_matrix[0] = np.zeros(embedding_dim)\n",
    "    \n",
    "    # Track statistics\n",
    "    found_words = 0\n",
    "    \n",
    "    # Read GloVe file and populate embedding matrix\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "            # Each line is: word vec[0] vec[1] ... vec[embedding_dim-1]\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            \n",
    "            # Check if this word is in our vocabulary\n",
    "            if word in word2idx:\n",
    "                # Extract embedding vector (convert strings to floats)\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                \n",
    "                # Store in our embedding matrix at the correct index\n",
    "                idx = word2idx[word]\n",
    "                embedding_matrix[idx] = vector\n",
    "                found_words += 1\n",
    "    \n",
    "    # Print coverage statistics\n",
    "    coverage = (found_words / vocab_size) * 100\n",
    "    print(f\"\\nGloVe Coverage:\")\n",
    "    print(f\"  Words in vocabulary: {vocab_size:,}\")\n",
    "    print(f\"  Words found in GloVe: {found_words:,}\")\n",
    "    print(f\"  Coverage: {coverage:.2f}%\")\n",
    "    print(f\"  Words using random initialization: {vocab_size - found_words:,}\")\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Configuration: Set this to your GloVe file path\n",
    "# You can download GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# For this project, we use glove.6B.100d.txt (100-dimensional vectors)\n",
    "GLOVE_PATH = \"../data/glove.6B.100d.txt\"  # Adjust this path as needed\n",
    "EMBEDDING_DIM = 100  # Must match the GloVe file (50, 100, 200, or 300)\n",
    "\n",
    "# Check if GloVe file exists\n",
    "if os.path.exists(GLOVE_PATH):\n",
    "    # Load GloVe embeddings for our vocabulary\n",
    "    glove_embeddings = load_glove_embeddings(GLOVE_PATH, word2idx, EMBEDDING_DIM)\n",
    "    print(f\"\\nGloVe embedding matrix shape: {glove_embeddings.shape}\")\n",
    "    print(f\"Expected shape: ({len(word2idx)}, {EMBEDDING_DIM})\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  GloVe file not found at: {GLOVE_PATH}\")\n",
    "    print(\"\\nPlease download GloVe embeddings:\")\n",
    "    print(\"1. Visit: https://nlp.stanford.edu/projects/glove/\")\n",
    "    print(\"2. Download: glove.6B.zip (822 MB)\")\n",
    "    print(\"3. Extract glove.6B.100d.txt to ../data/\")\n",
    "    print(\"4. Update GLOVE_PATH variable above if using a different location\")\n",
    "    print(\"\\nAlternatively, you can train with random embeddings only.\")\n",
    "    glove_embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create PyTorch Dataset Class\n",
    "\n",
    "This class handles:\n",
    "- Converting words to numbers\n",
    "- Converting tags to numbers\n",
    "- Padding sequences to the same length\n",
    "- Creating attention masks (to ignore padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Named Entity Recognition.\n",
    "    \n",
    "    Converts tokens and tags to numerical format and handles padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, word2idx, tag2idx, max_len=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: List of dicts with 'tokens' and 'ner_tags'\n",
    "            word2idx: Dictionary mapping words to indices\n",
    "            tag2idx: Dictionary mapping tags to indices\n",
    "            max_len: Maximum sequence length (longer sequences are truncated)\n",
    "        \"\"\"\n",
    "        self.data = data  # Store the data\n",
    "        self.word2idx = word2idx  # Store word mappings\n",
    "        self.tag2idx = tag2idx  # Store tag mappings\n",
    "        self.max_len = max_len  # Store max length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single example from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - input_ids: Token indices (padded to max_len)\n",
    "                - labels: Tag indices (padded to max_len)\n",
    "                - attention_mask: 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        # Get the example at this index\n",
    "        example = self.data[idx]\n",
    "        tokens = example['tokens']  # List of words\n",
    "        tags = example['ner_tags']  # List of NER tags\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        # Use <UNK> index for words not in vocabulary\n",
    "        token_ids = [\n",
    "            self.word2idx.get(token.lower(), self.word2idx['<UNK>']) \n",
    "            for token in tokens\n",
    "        ]\n",
    "        \n",
    "        # Convert tags to indices\n",
    "        tag_ids = [self.tag2idx[tag] for tag in tags]\n",
    "        \n",
    "        # Truncate or pad to max_len\n",
    "        if len(token_ids) > self.max_len:\n",
    "            # Sequence too long: truncate\n",
    "            token_ids = token_ids[:self.max_len]\n",
    "            tag_ids = tag_ids[:self.max_len]\n",
    "        else:\n",
    "            # Sequence too short: pad with <PAD> (index 0)\n",
    "            padding_len = self.max_len - len(token_ids)\n",
    "            token_ids = token_ids + [self.word2idx['<PAD>']] * padding_len\n",
    "            tag_ids = tag_ids + [0] * padding_len  # Pad tags with 0 (usually 'O')\n",
    "        \n",
    "        # Create attention mask: 1 for real tokens, 0 for padding\n",
    "        # The model will ignore positions with 0 in the mask\n",
    "        attention_mask = [\n",
    "            1 if tid != self.word2idx['<PAD>'] else 0 \n",
    "            for tid in token_ids\n",
    "        ]\n",
    "        \n",
    "        # Return as PyTorch tensors\n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),  # Token indices\n",
    "            'labels': torch.tensor(tag_ids, dtype=torch.long),  # Tag indices\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)  # Mask\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETERS: You can easily adjust these values\n",
    "# =============================================================================\n",
    "MAX_LEN = 128        # Maximum sequence length (adjust based on sentence length analysis)\n",
    "BATCH_SIZE = 16      # Number of examples per batch (increase if you have more memory)\n",
    "\n",
    "# Create dataset objects for train, validation, and test splits\n",
    "train_dataset = NERDataset(train_data, word2idx, tag2idx, MAX_LEN)\n",
    "val_dataset = NERDataset(val_data, word2idx, tag2idx, MAX_LEN)\n",
    "test_dataset = NERDataset(test_data, word2idx, tag2idx, MAX_LEN)\n",
    "\n",
    "# Create data loaders (handle batching and shuffling)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True  # Shuffle training data each epoch\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False  # Don't shuffle validation data\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False  # Don't shuffle test data\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Training:   {len(train_dataset):4d} sentences, {len(train_loader):3d} batches\")\n",
    "print(f\"  Validation: {len(val_dataset):4d} sentences, {len(val_loader):3d} batches\")\n",
    "print(f\"  Test:       {len(test_dataset):4d} sentences, {len(test_loader):3d} batches\")\n",
    "\n",
    "# Test: Get one batch and show its shape\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  input_ids:      {sample_batch['input_ids'].shape}  (batch_size, max_len)\")\n",
    "print(f\"  labels:         {sample_batch['labels'].shape}  (batch_size, max_len)\")\n",
    "print(f\"  attention_mask: {sample_batch['attention_mask'].shape}  (batch_size, max_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define BiLSTM Model Architecture\n",
    "\n",
    "This model is designed to be **highly configurable** for easy experimentation:\n",
    "\n",
    "**Architecture**:\n",
    "1. **Embedding layer**: Converts word indices to dense vectors\n",
    "   - Can use random initialization or pre-trained GloVe\n",
    "   - Can be frozen or fine-tuned during training\n",
    "2. **BiLSTM layer**: Processes sequence in both directions (forward and backward)\n",
    "   - Number of layers is configurable\n",
    "   - Hidden dimension is configurable\n",
    "3. **Dropout**: Regularization to prevent overfitting\n",
    "4. **Classification layer**: Predicts NER tag for each token\n",
    "\n",
    "**Key Parameters for Experimentation**:\n",
    "- `embedding_dim`: Size of word embeddings (e.g., 50, 100, 200, 300)\n",
    "- `hidden_dim`: Size of LSTM hidden state (e.g., 64, 128, 256)\n",
    "- `num_layers`: Number of stacked LSTM layers (e.g., 1, 2, 3)\n",
    "- `dropout`: Dropout probability (e.g., 0.1, 0.3, 0.5)\n",
    "- `pretrained_embeddings`: Optional pre-trained embedding matrix\n",
    "- `freeze_embeddings`: Whether to freeze embeddings during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for sequence labeling (NER).\n",
    "    \n",
    "    Architecture:\n",
    "        Input (token ids) → Embedding → BiLSTM → Dropout → Linear → Output (tag logits)\n",
    "    \n",
    "    This model is designed for easy experimentation with different configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 num_tags,\n",
    "                 num_layers=2, \n",
    "                 dropout=0.3,\n",
    "                 pretrained_embeddings=None,\n",
    "                 freeze_embeddings=False):\n",
    "        \"\"\"\n",
    "        Initialize the BiLSTM model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of word vocabulary\n",
    "            embedding_dim: Dimension of word embeddings (e.g., 100 for GloVe 100d)\n",
    "            hidden_dim: Dimension of LSTM hidden state (controls model capacity)\n",
    "            num_tags: Number of NER tags (output classes)\n",
    "            num_layers: Number of stacked LSTM layers (default=2)\n",
    "                       More layers can capture more complex patterns but may overfit\n",
    "            dropout: Dropout probability for regularization (default=0.3)\n",
    "                    Higher values prevent overfitting but may underfit\n",
    "            pretrained_embeddings: Optional pre-trained embedding matrix (NumPy array)\n",
    "                                  Shape: (vocab_size, embedding_dim)\n",
    "                                  If None, embeddings are randomly initialized\n",
    "            freeze_embeddings: If True, don't update embeddings during training\n",
    "                              Useful for preserving pre-trained semantic relationships\n",
    "        \"\"\"\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        \n",
    "        # Store configuration for later reference\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # =============================================================================\n",
    "        # EMBEDDING LAYER: Converts word indices to dense vectors\n",
    "        # =============================================================================\n",
    "        # padding_idx=0 means the padding token (index 0) has zero embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,      # Number of words in vocabulary\n",
    "            embedding_dim,   # Size of embedding vector for each word\n",
    "            padding_idx=0    # Padding token index (will have zero vector)\n",
    "        )\n",
    "        \n",
    "        # If pre-trained embeddings are provided, load them\n",
    "        if pretrained_embeddings is not None:\n",
    "            print(\"  Loading pre-trained embeddings...\")\n",
    "            # Convert NumPy array to PyTorch tensor\n",
    "            self.embedding.weight = nn.Parameter(\n",
    "                torch.tensor(pretrained_embeddings, dtype=torch.float32)\n",
    "            )\n",
    "            # Optionally freeze embeddings (don't update during training)\n",
    "            if freeze_embeddings:\n",
    "                print(\"  Freezing embedding weights (will not be updated during training)\")\n",
    "                self.embedding.weight.requires_grad = False\n",
    "            else:\n",
    "                print(\"  Embeddings will be fine-tuned during training\")\n",
    "        else:\n",
    "            print(\"  Using randomly initialized embeddings\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # DROPOUT LAYER: Regularization to prevent overfitting\n",
    "        # =============================================================================\n",
    "        # Randomly sets a fraction of inputs to 0 during training\n",
    "        # This prevents the model from relying too much on any single feature\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # BiLSTM LAYER: Processes sequence in both directions\n",
    "        # =============================================================================\n",
    "        # LSTM (Long Short-Term Memory) handles long-range dependencies better than simple RNN\n",
    "        # Bidirectional means it processes the sequence both forward and backward\n",
    "        # This gives the model context from both past and future tokens\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,           # Input size (size of word embeddings)\n",
    "            hidden_dim,              # Hidden state size (controls model capacity)\n",
    "            num_layers=num_layers,   # Stack multiple LSTM layers for more expressiveness\n",
    "            bidirectional=True,      # Process sequence in both directions\n",
    "            batch_first=True,        # Input shape: (batch, seq_len, features)\n",
    "            dropout=dropout if num_layers > 1 else 0  # Dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OUTPUT LAYER: Maps BiLSTM output to tag scores\n",
    "        # =============================================================================\n",
    "        # BiLSTM outputs are concatenated (forward + backward), so output dim is 2 * hidden_dim\n",
    "        # This layer produces a score for each possible NER tag for each token\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim * 2,  # Input: BiLSTM output (forward + backward)\n",
    "            num_tags         # Output: Score for each NER tag\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token indices, shape (batch_size, seq_len)\n",
    "            attention_mask: Mask for padding, shape (batch_size, seq_len)\n",
    "                           1 for real tokens, 0 for padding\n",
    "        \n",
    "        Returns:\n",
    "            logits: Tag scores for each token, shape (batch_size, seq_len, num_tags)\n",
    "                   Higher score = more confident prediction for that tag\n",
    "        \"\"\"\n",
    "        # Step 1: Convert token indices to embeddings\n",
    "        # Shape: (batch_size, seq_len) → (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # Apply dropout to embeddings for regularization\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        \n",
    "        # Step 2: Process sequence with BiLSTM\n",
    "        # lstm_out contains the output from both forward and backward passes\n",
    "        # Shape: (batch_size, seq_len, embedding_dim) → (batch_size, seq_len, hidden_dim * 2)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Apply dropout to LSTM output\n",
    "        lstm_out = self.dropout_layer(lstm_out)\n",
    "        \n",
    "        # Step 3: Predict tag for each token\n",
    "        # Maps LSTM output to tag scores\n",
    "        # Shape: (batch_size, seq_len, hidden_dim * 2) → (batch_size, seq_len, num_tags)\n",
    "        logits = self.fc(lstm_out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Return model configuration for logging and reproducibility.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'num_layers': self.num_layers,\n",
    "            'dropout': self.dropout,\n",
    "            'bidirectional': True  # Always True for BiLSTM\n",
    "        }\n",
    "\n",
    "print(\"BiLSTM model class defined successfully!\")\n",
    "print(\"\\nYou can easily experiment with different architectures by changing:\")\n",
    "print(\"  - embedding_dim: Size of word vectors (50, 100, 200, 300)\")\n",
    "print(\"  - hidden_dim: LSTM hidden state size (64, 128, 256, 512)\")\n",
    "print(\"  - num_layers: Number of LSTM layers (1, 2, 3, 4)\")\n",
    "print(\"  - dropout: Regularization strength (0.1, 0.3, 0.5)\")\n",
    "print(\"  - pretrained_embeddings: Use GloVe or random initialization\")\n",
    "print(\"  - freeze_embeddings: Freeze or fine-tune pre-trained embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Models with Different Embeddings\n",
    "\n",
    "We'll create two BiLSTM models with identical architecture but different embedding strategies:\n",
    "\n",
    "1. **BiLSTM with Random Embeddings**: \n",
    "   - Embeddings initialized randomly\n",
    "   - Learned from scratch during training\n",
    "   - No prior knowledge of word meanings\n",
    "\n",
    "2. **BiLSTM with GloVe Embeddings**:\n",
    "   - Embeddings initialized with pre-trained GloVe vectors\n",
    "   - Fine-tuned during training (not frozen)\n",
    "   - Benefits from semantic knowledge learned on massive text corpora\n",
    "\n",
    "**Note**: All other hyperparameters are kept identical for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL HYPERPARAMETERS: Easy to adjust for experimentation\n",
    "# =============================================================================\n",
    "EMBEDDING_DIM = 100   # Dimension of word embeddings (must match GloVe if using pre-trained)\n",
    "HIDDEN_DIM = 128      # Size of LSTM hidden state (controls model capacity)\n",
    "NUM_LAYERS = 2        # Number of stacked LSTM layers\n",
    "DROPOUT = 0.3         # Dropout probability (0.0 = no dropout, 0.5 = aggressive)\n",
    "NUM_TAGS = len(tag_names)  # Number of output classes (NER tags)\n",
    "VOCAB_SIZE = len(vocab)    # Size of vocabulary\n",
    "\n",
    "# Whether to freeze GloVe embeddings or fine-tune them\n",
    "FREEZE_GLOVE = False  # Set to True to prevent updating GloVe embeddings during training\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Vocabulary size:     {VOCAB_SIZE:,}\")\n",
    "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Hidden dimension:    {HIDDEN_DIM}\")\n",
    "print(f\"  Number of layers:    {NUM_LAYERS}\")\n",
    "print(f\"  Dropout:             {DROPOUT}\")\n",
    "print(f\"  Number of tags:      {NUM_TAGS}\")\n",
    "print(f\"  Freeze GloVe:        {FREEZE_GLOVE}\")\n",
    "\n",
    "# Dictionary to store our models\n",
    "models = {}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL 1: BiLSTM with Random Embeddings (Baseline)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model 1: BiLSTM with Random Embeddings\")\n",
    "print(\"-\"*80)\n",
    "models['BiLSTM_Random'] = BiLSTMTagger(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_tags=NUM_TAGS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pretrained_embeddings=None,  # No pre-trained embeddings\n",
    "    freeze_embeddings=False       # Not applicable for random init\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL 2: BiLSTM with GloVe Embeddings\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Model 2: BiLSTM with GloVe Embeddings\")\n",
    "print(\"-\"*80)\n",
    "if glove_embeddings is not None:\n",
    "    models['BiLSTM_GloVe'] = BiLSTMTagger(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_tags=NUM_TAGS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pretrained_embeddings=glove_embeddings,  # Use GloVe embeddings\n",
    "        freeze_embeddings=FREEZE_GLOVE           # Fine-tune or freeze\n",
    "    )\n",
    "else:\n",
    "    print(\"  ⚠️  GloVe embeddings not loaded. Skipping this model.\")\n",
    "    print(\"  Please load GloVe embeddings in the previous cell to train this model.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Print Model Statistics\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Count total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    # Count trainable parameters (may be less if embeddings are frozen)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Show if any parameters are frozen\n",
    "    if total_params != trainable_params:\n",
    "        frozen_params = total_params - trainable_params\n",
    "        print(f\"  Frozen parameters:    {frozen_params:,} ({frozen_params/total_params*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Successfully initialized {len(models)} model(s)!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}